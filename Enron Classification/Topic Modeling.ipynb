{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#forked from JayKrishna on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df120470-b6a6-d1c5-2ecc-b7129b2466dd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, email,re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set_style('whitegrid')\n",
    "import wordcloud\n",
    "\n",
    "# Network analysis\n",
    "import networkx as nx\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2a6492f3-ffbc-93d2-1135-1f3e23088f8a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data into a DataFrame\n",
    "emails_df = pd.read_csv('../input/emails.csv')\n",
    "print(emails_df.shape)\n",
    "emails_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c55ba68d-6955-0d73-3aca-9f33bfc870ab",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "def get_text_from_email(msg):\n",
    "    '''To get the content from email objects'''\n",
    "    parts = []\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            parts.append( part.get_payload() )\n",
    "    return ''.join(parts)\n",
    "\n",
    "def split_email_addresses(line):\n",
    "    '''To separate multiple email addresses'''\n",
    "    if line:\n",
    "        addrs = line.split(',')\n",
    "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
    "    else:\n",
    "        addrs = None\n",
    "    return addrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4d7327aa-adfb-aee3-d4f1-e6631a7ddc9e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the emails into a list email objects\n",
    "messages = list(map(email.message_from_string, emails_df['message']))\n",
    "emails_df.drop('message', axis=1, inplace=True)\n",
    "# Get fields from parsed email objects\n",
    "keys = messages[0].keys()\n",
    "for key in keys:\n",
    "    emails_df[key] = [doc[key] for doc in messages]\n",
    "# Parse content from emails\n",
    "emails_df['content'] = list(map(get_text_from_email, messages))\n",
    "# Split multiple email addresses\n",
    "emails_df['From'] = emails_df['From'].map(split_email_addresses)\n",
    "emails_df['To'] = emails_df['To'].map(split_email_addresses)\n",
    "\n",
    "# Extract the root of 'file' as 'user'\n",
    "emails_df['user'] = emails_df['file'].map(lambda x:x.split('/')[0])\n",
    "del messages\n",
    "\n",
    "emails_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1d641bec-4605-75c9-da9e-3d8cc4a64dfa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set index and drop columns with two few values\n",
    "emails_df = emails_df.set_index('Message-ID')\\\n",
    "    .drop(['file', 'Mime-Version', 'Content-Type', 'Content-Transfer-Encoding'], axis=1)\n",
    "# Parse datetime\n",
    "emails_df['Date'] = pd.to_datetime(emails_df['Date'], infer_datetime_format=True)\n",
    "emails_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6e42e883-9417-a7db-c77e-9b59e78b3c52",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\",\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    porter= PorterStemmer()\n",
    "    \n",
    "    text=text.rstrip()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    #stem = \" \".join(porter.stem(token) for token in normalized.split())\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5fc9545f-2207-8d5e-8380-58997549c84b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysis_df=emails_df[['From', 'To', 'Date','content']].dropna().copy()\n",
    "analysis_df = analysis_df.loc[analysis_df['To'].map(len) == 1]\n",
    "sub_df=analysis_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b62b77a3-fc79-b883-8eb3-67c4a77baed8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sub_df[\"content\"]=sub_df[\"content\"].map(clean)\n",
    "text_clean=[]\n",
    "for text in sub_df['content']:\n",
    "    text_clean.append(clean(text).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "89ab4b9a-9132-6fea-17b7-135f1dc00a7c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(text_clean)\n",
    "text_term_matrix = [dictionary.doc2bow(text) for text in text_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "16714ab8-100b-93da-5ef9-1991e59a0668",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(text_term_matrix, num_topics=4, id2word = dictionary, passes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4a22fdc9-7bc1-aabe-686e-3c01d638978d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    #text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    words = [w for w in words if w not in eng_stopwords]\n",
    "    return ' '.join(words)\n",
    "\n",
    "analysis_df[\"clean_content\"]=analysis_df.content.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a2a311d8-f00a-f085-633b-dc0a64ad32d1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordvector = TfidfVectorizer(analyzer='word', stop_words='english', max_df=0.4, min_df=5)\n",
    "short_analysis=analysis_df.sample(5000)\n",
    "wordvector_fit = wordvector.fit_transform(short_analysis.clean_content)\n",
    "feature = wordvector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43ee9d65-133c-c4ba-f6cf-37f1c4571ca6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 4\n",
    "clf = KMeans(n_clusters=N, \n",
    "            max_iter=50, \n",
    "            init='k-means++', \n",
    "            n_init=1)\n",
    "labels = clf.fit_predict(wordvector_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ce515e80-b3a2-183a-f20b-ab3c4360c015",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordvector_fit_2d = wordvector_fit.todense()\n",
    "pca = PCA(n_components=2).fit(wordvector_fit_2d)\n",
    "datapoint = pca.transform(wordvector_fit_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eabec3f2-37aa-6a05-cf33-5aef39126c16",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = [\"#e05f14\", \"#e0dc14\", \"#2fe014\", \"#14d2e0\"]\n",
    "color = [label[i] for i in labels]\n",
    "plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "\n",
    "centroids = clf.cluster_centers_\n",
    "centroidpoint = pca.transform(centroids)\n",
    "plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9cf4d20f-23e8-b18a-937c-850a57be9fcc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(ldamodel.print_topics(num_topics=4, num_words=10))\n",
    "print([(0, '0.012*\"enron\" + 0.012*\"deal\" + 0.010*\"agreement\" + 0.008*\"change\" + 0.008*\"contract\" + 0.008*\"corp\" + 0.007*\"fax\" + 0.005*\"houston\" + 0.005*\"date\" + 0.005*\"america\"'), (1, '0.005*\"message\" + 0.005*\"origin\" + 0.004*\"pleas\" + 0.004*\"email\" + 0.004*\"thank\" + 0.003*\"attach\" + 0.003*\"file\" + 0.003*\"copy\" + 0.003*\"inform\" + 0.003*\"receive\"'), (2, '0.015*\"thank\" + 0.008*\"call\" + 0.005*\"time\" + 0.004*\"meet\" + 0.003*\"look\" + 0.003*\"week\" + 0.003*\"day\" + 0.003*\"lunch\" + 0.003*\"talk\" + 0.003*\"hello\"'), (3, '0.016*\"market\" + 0.009*\"gas\" + 0.008*\"price\" + 0.005*\"power\" + 0.004*\"company\" + 0.004*\"energy\" + 0.003*\"business\" + 0.003*\"service\" + 0.003*\"manage\" + 0.003*\"fare\"')])"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
